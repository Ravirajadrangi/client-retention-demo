{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "urth": {
     "dashboard": {
      "hidden": true
     }
    }
   },
   "source": [
    "# Client Retention Demo\n",
    "\n",
    "## Setup Environment\n",
    "**Install the Rocket JDBC driver**\n",
    "\n",
    "We need to install the Rocket JDBC driver in order to access data from DB2 and VSAM through MDS.\n",
    ">Ideally, for scalability, this file should be hosted on a web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Install the Rocket Driver\n",
    "%addjar file:///home/jovyan/work/dv-jdbc-3.1.22510.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install MongoDB and Logging Drivers**\n",
    "\n",
    "We need to install the MongoDB and Logging Drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Install MongoDB and Log Jars\n",
    "%addjar http://central.maven.org/maven2/com/stratio/datasource/spark-mongodb_2.10/0.11.0/spark-mongodb_2.10-0.11.0.jar\n",
    "%addjar http://central.maven.org/maven2/org/mongodb/casbah-commons_2.10/2.8.0/casbah-commons_2.10-2.8.0.jar\n",
    "%addjar http://central.maven.org/maven2/org/mongodb/casbah-core_2.10/2.8.0-RC0/casbah-core_2.10-2.8.0-RC0.jar\n",
    "%addjar http://central.maven.org/maven2/org/mongodb/casbah-query_2.10/2.8.0/casbah-query_2.10-2.8.0.jar\n",
    "%addjar http://central.maven.org/maven2/org/mongodb/mongo-java-driver/2.13.0/mongo-java-driver-2.13.0.jar\n",
    "%addjar http://central.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.4.1/log4j-api-2.4.1.jar\n",
    "%addjar http://central.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.4.1/log4j-core-2.4.1.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reload Java Classpath**\n",
    "\n",
    "Now that the new jars have been downloaded, we need to reload the classpath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@transient val systemClassLoader = ClassLoader.getSystemClassLoader().asInstanceOf[java.net.URLClassLoader]\n",
    "@transient val m = classOf[java.net.URLClassLoader].getDeclaredMethod(\"addURL\", classOf[java.net.URL])\n",
    "m.setAccessible(true)\n",
    "\n",
    "def load_jar(myUrl: java.net.URL) = {\n",
    "    m.invoke(systemClassLoader, myUrl)\n",
    "}\n",
    "\n",
    "kernel.interpreter.classLoader.asInstanceOf[java.net.URLClassLoader].getURLs.foreach(load_jar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Connection Variables\n",
    "In this section, we need to specify all of the connection variables that will be used later in the Notebook.  Some of these variables may have been written as system environment variables.  If they were, those variables can be left as is.  If not, we will need to replace the 'sys.env([env_name])' with a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Environment Specific Settings\n",
    "val jdbcUser = sys.env(\"JDBC_USER\")\n",
    "val jdbcPass = sys.env(\"JDBC_PASS\")\n",
    "val jdbcHost = sys.env(\"JDBC_HOST\")\n",
    "\n",
    "val mongoUser = sys.env(\"MONGO_USER\")\n",
    "val mongoPass = sys.env(\"MONGO_PASS\")\n",
    "val mongoHost = sys.env(\"MONGO_HOST\")\n",
    "val mongoDB = \"demo\"\n",
    "val mongoColl = \"client_features\"\n",
    "print(jdbcHost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Import Apache Spark and MongoDB Classes\n",
    "In this section we need to import all of the necessary classes for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlContext.implicits._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import com.mongodb.casbah.Imports._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access MDS Data\n",
    "In the following section we will access DB2 and VSAM data stored on the z/OS host using the Rocket jdbc MDS driver.\n",
    "\n",
    "**Create an SQL Context**\n",
    "\n",
    "We first need to create an SQL Context to be used in accessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Client Income Data Stored in a VSAM File\n",
    "In the following section, we will load client inforamtion stored in a VSAM Data Set as a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var url = \"jdbc:rs:dv://\" + jdbcHost + \":1200;DSN=AZKS; setLevel=-1; DBMD=GRAPHIC; DBTY=DVS;\" \n",
    "url += \" HOST=\" + jdbcHost + \"; LGID=ENC; LoginTimeOut=10; PLAN=SDBC1010; PORT=1200;\" \n",
    "url += \" PWD=\" + jdbcPass + \"; SUBSYS=NONE; UID=\" + jdbcUser + \"; enableCancel=True; queryTimeout=30;\"\n",
    "\n",
    "var dbtable = \"(SELECT\" \n",
    "dbtable += \" *\"\n",
    "dbtable += \" FROM AZKSQL.CLIENT_INFO_VSAMKSDS)as customerRows\"\n",
    "\n",
    "val clientIncome_df = sqlContext.read.format(\"jdbc\").options(Map(\n",
    "        \"driver\" -> \"com.rs.jdbc.dv.DvDriver\",\n",
    "        \"url\" -> url,\n",
    "        \"dbtable\" -> dbtable)).load() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Client Transaction Data Stored in a DB2 Table\n",
    "In the following section, we will load client transaction data stored in a DB2 table as a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var url = \"jdbc:rs:dv://\" + jdbcHost + \":1200;DSN=AZKS; setLevel=-1; DBMD=GRAPHIC; DBTY=DVS;\" \n",
    "url += \" HOST=\" + jdbcHost + \"; LGID=ENC; LoginTimeOut=10; PLAN=SDBC1010; PORT=1200;\"\n",
    "url += \" PWD=\" + jdbcPass + \"; SUBSYS=NONE; UID=\" + jdbcUser + \"; enableCancel=True; queryTimeout=30;\"\n",
    "\n",
    "var dbtable = \"(SELECT\"\n",
    "dbtable += \" *\" \n",
    "dbtable += \" FROM AZKSQL.SPPAYTB)as customerRows\"\n",
    "\n",
    "val clientTrans_df = sqlContext.read.format(\"jdbc\").options(Map(\n",
    "        \"driver\" -> \"com.rs.jdbc.dv.DvDriver\",\n",
    "        \"url\" -> url,\n",
    "        \"dbtable\" -> dbtable)).load()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Aggregate Statistics Using Apache Spark\n",
    "In this section, we will compute a few aggregate statistics leveraging the Spark DataFrame API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val calcTrans_df = clientTrans_df.groupBy(\"CONT_ID\").agg(\n",
    "    sum(\"ACAUREQ_AUREQ_TX_DT_TTLAMT\").cast(\"float\").as(\"total_txn_amount\"),\n",
    "    (count(\"ACAUREQ_AUREQ_TX_DT_TTLAMT\")/365).cast(\"float\").as(\"avg_daily_txns\"),\n",
    "    count(\"ACAUREQ_AUREQ_TX_DT_TTLAMT\").cast(\"int\").as(\"total_txns\"),\n",
    "    (sum(\"ACAUREQ_AUREQ_TX_DT_TTLAMT\")/count(\"ACAUREQ_AUREQ_TX_DT_TTLAMT\")).cast(\"float\").as(\"avg_txn_amount\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the Computed Aggregate Statistics with the Client Information\n",
    "In this section, we will join the newly computed aggregate statistics, built off of the client transaction data, with the client information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val client_df = clientIncome_df.select(\n",
    "    $\"CONT_ID\".cast(\"int\").as(\"customer_id\"),\n",
    "    $\"GENDER\".cast(\"int\").as(\"gender\"),\n",
    "    $\"AGE_YEARS\".cast(\"float\").as(\"age_years\"),\n",
    "    $\"HIGHEST_EDU\".cast(\"int\").as(\"highest_edu\"),\n",
    "    $\"ANNUAL_INVEST\".cast(\"float\").as(\"annual_investment_rev\"),\n",
    "    $\"ANNUAL_INCOME\".cast(\"float\").as(\"annual_income\"),\n",
    "    $\"ACTIVITY_LEVEL\".cast(\"int\").as(\"activity_level\"),\n",
    "    $\"CHURN\".cast(\"int\").as(\"churn\")\n",
    "    ).join(calcTrans_df.select(\n",
    "    $\"CONT_ID\", \n",
    "    $\"total_txn_amount\",\n",
    "    $\"avg_daily_txns\",\n",
    "    $\"total_txns\",\n",
    "    $\"avg_txn_amount\"\n",
    "    ), $\"CONT_ID\" === $\"customer_id\", \"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to MongoDB\n",
    "In this section we will collect the contents of our DataFrame and write it out to a MongoDB collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// connect to MongoDB and drop the mongo collection specified above\n",
    "val uri = MongoClientURI(\"mongodb://\" + mongoUser + \":\" + mongoPass + \"@\" + mongoHost + \"/?authMechanism=SCRAM-SHA-1\")\n",
    "val mongoClient =  MongoClient(uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup a Function that will Load a DataFrame Contents into a MongoDB Collection**\n",
    "\n",
    "In this section we will setup a function that will write the contents of a DataFrame into a MongoDB collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dfToMongo ( df : DataFrame, coll : MongoCollection ) {\n",
    "    // collect the DataFrame's schema in List form\n",
    "    val schema = df.schema.map(f => s\"${f.name}\")\n",
    "\n",
    "    // iterate through a Mapped version of the DataFrame using the value 'schema'\n",
    "    // to create the map, then write the value to the MongoDB collection\n",
    "    for ( row <- df.map(_.getValuesMap[Any]( schema )).collect() ) {\n",
    "        val builder = MongoDBObject.newBuilder\n",
    "        for ( field <- schema ){\n",
    "            if ( row(field) != null ) {\n",
    "                if ( field != \"CONT_ID\" ) {\n",
    "                    builder += field -> row(field)\n",
    "                }\n",
    "            } else if ( field == \"annual_investment_rev\" ) {\n",
    "                builder += field -> 0.toFloat\n",
    "            }\n",
    "        } \n",
    "        coll.insert(builder.result)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop MongoDB Collection to Prepare for New Data**\n",
    "\n",
    "Because this is a demo environment, we need to ensure the MongoDB collection is empty, so we need to first drop the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mongoClient(mongoDB)(mongoColl).drop()\n",
    "mongoClient(mongoDB)(mongoColl).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the Client DataFrame into MongoDB**\n",
    "\n",
    "In this section we will write our DataFrame contents into a MongoDB collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfToMongo(client_df, mongoClient(mongoDB)(mongoColl))\n",
    "mongoClient(mongoDB)(mongoColl).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10.4 with v1.1.0 IBM z/OS Platform for Apache Spark",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  },
  "urth": {
   "dashboard": {
    "cellMargin": 10,
    "defaultCellHeight": 20,
    "maxColumns": 12
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
